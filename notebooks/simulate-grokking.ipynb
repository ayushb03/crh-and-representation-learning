{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from itertools import permutations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal transformer block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(dim)\n",
    "        self.ln_2 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_mask = torch.full(\n",
    "            (len(x), len(x)), -float(\"Inf\"), device=x.device, dtype=x.dtype\n",
    "        )\n",
    "        attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "\n",
    "        x = self.ln_1(x)\n",
    "        a, _ = self.attn(x, x, x, attn_mask=attn_mask, need_weights=False)\n",
    "        x = x + a\n",
    "        m = self.mlp(self.ln_2(x))\n",
    "        x = x + m\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal Transformer decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=128, num_layers=2, num_heads=4, num_tokens=97, seq_len=5):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(num_tokens, dim)\n",
    "        self.position_embeddings = nn.Embedding(seq_len, dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(Block(dim, num_heads))\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, num_tokens, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.token_embeddings(x)\n",
    "        positions = torch.arange(x.shape[0], device=x.device).unsqueeze(-1)\n",
    "        h = h + self.position_embeddings(positions).expand_as(h)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "\n",
    "        h = self.ln_f(h)\n",
    "        logits = self.head(h)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def division_mod_p_data(p, eq_token, op_token):\n",
    "    \"\"\"\n",
    "    x◦y = x/y (mod p) for 0 ≤ x < p, 0 < y < p\n",
    "    \"\"\"\n",
    "    x = torch.arange(p)\n",
    "    y = torch.arange(1, p)\n",
    "    x, y = torch.cartesian_prod(x, y).T\n",
    "\n",
    "    eq = torch.ones_like(x) * eq_token\n",
    "    op = torch.ones_like(x) * op_token\n",
    "    result = x * y % p\n",
    "\n",
    "    # \"All of our experiments used a small transformer trained on datasets of\n",
    "    # equations of the form a◦b = c, where each of “a”, “◦”, “b”, “=”, and “c”\n",
    "    # is a seperate token\"\n",
    "    return torch.stack([x, op, y, eq, result])\n",
    "\n",
    "\n",
    "def train_transformer(p=97, budget=3e5, batch_size=512, lr=1e-3, beta1=0.9, beta2=0.98, weight_decay=0, optimizer=\"Adam\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    eq_token = p\n",
    "    op_token = p + 1\n",
    "\n",
    "    model = Decoder(\n",
    "        dim=128, num_layers=2, num_heads=4, num_tokens=p + 2, seq_len=5\n",
    "    ).to(device)\n",
    "\n",
    "    data = division_mod_p_data(p, eq_token, op_token)\n",
    "    train_idx, valid_idx = torch.randperm(data.shape[1]).split(data.shape[1] // 2)\n",
    "    train_data, valid_data = data[:, train_idx], data[:, valid_idx]\n",
    "\n",
    "    optimizer = getattr(torch.optim, optimizer)(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        betas=(beta1, beta2),\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, lambda update: 1 if update > 10 else update / 10\n",
    "    )\n",
    "\n",
    "    steps_per_epoch = math.ceil(train_data.shape[1] / batch_size)\n",
    "\n",
    "    train_acc, val_acc, train_loss, val_loss = [], [], [], []\n",
    "\n",
    "    for e in tqdm(range(int(budget) // steps_per_epoch)):\n",
    "\n",
    "        train_data = train_data[:, torch.randperm(train_data.shape[1])]\n",
    "\n",
    "        for data, is_train in [(train_data, True), (valid_data, False)]:\n",
    "\n",
    "            model.train(is_train)\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "\n",
    "            dl = torch.split(data, batch_size, dim=1)\n",
    "            for input in dl:\n",
    "                input = input.to(device)\n",
    "\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits = model(input[:-1])\n",
    "                    loss = F.cross_entropy(logits[-1], input[-1])\n",
    "                    total_loss += loss.item() * input.shape[-1]\n",
    "\n",
    "                if is_train:\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                acc = (logits[-1].argmax(-1) == input[-1]).float().mean()\n",
    "                total_acc += acc.item() * input.shape[-1]\n",
    "\n",
    "            if is_train:\n",
    "                train_acc.append(total_acc / train_data.shape[-1])\n",
    "                train_loss.append(total_loss / train_data.shape[-1])\n",
    "            else:\n",
    "                val_acc.append(total_acc / valid_data.shape[-1])\n",
    "                val_loss.append(total_loss / valid_data.shape[-1])\n",
    "\n",
    "        if (e + 1) % 100 == 0:\n",
    "            steps = torch.arange(len(train_acc)).numpy() * steps_per_epoch\n",
    "            plt.plot(steps, train_acc, label=\"train\")\n",
    "            plt.plot(steps, val_acc, label=\"val\")\n",
    "            plt.legend()\n",
    "            plt.title(\"Modular Division (training on 50% of data)\")\n",
    "            plt.xlabel(\"Optimization Steps\")\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.xscale(\"log\", base=10)\n",
    "            plt.savefig(\"figures/acc.png\", dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "            plt.plot(steps, train_loss, label=\"train\")\n",
    "            plt.plot(steps, val_loss, label=\"val\")\n",
    "            plt.legend()\n",
    "            plt.title(\"Modular Division (training on 50% of data)\")\n",
    "            plt.xlabel(\"Optimization Steps\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.xscale(\"log\", base=10)\n",
    "            plt.savefig(\"figures/loss.png\", dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "train_transformer(p=97, budget=3e5, batch_size=512, lr=1e-3, beta1=0.9, beta2=0.98, weight_decay=0, optimizer=\"Adam\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
